{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d38b89d",
   "metadata": {},
   "source": [
    "# Project 6d - Inferring Material Properties\n",
    "As we have seen before, it's often the case that we would like to model a system without having full knowledge of the properties of that system, and that such properties will need to be inferred from data.  That is very often true of partial differential equations of many types.  \n",
    "\n",
    "Take the following as an example: we would like to know the thermal diffusivity $k$ of some new material.  Our testing apparatus is such that we can specify the temperature at one end (a Dirichlet boundary condition of, say, $u(x=0,t) = 1$, and we can perfectly insulate the other end (the Neumann condition $\\frac{\\partial u}{\\partial x}\\left(x=1,t\\right) = 0$).  We measure the temperature (with a normally-distributed error of $\\sigma=0.05$) at the insulated boundary at $t=5$ and find it to be $u_{obs}(x=1,t=5) = 0.63$.  What potential thermal diffusivities are compatible with this observation?  \n",
    "\n",
    "**Adapt the MCMC procedure that we used to calibrate SIR models to answer this question.**  Note that I have reproduced the Metropolis algorithm class and the SIR likelihood class to serve as a starting point.  Note that you will *not* need to modify the Metropolis class, but you *will* need to modify the SIR class to work with your diffusion equation solver instead, and to represent the observation scenario described above.  It is worth mentioning that you will have to run the diffusion solver to $t=5$ many times (on the order of tens of thousands of runs) with many different possible diffusivities.  As such, it will behoove you to use the largest time step (and $\\Delta x$) that you can that is still accurate.  I encourage you to use backward Euler - however you should compare computed solutions at the final time for different time steps (for some assumed $k$ and outside the framework of MCMC) to see how big a time step you can use while still getting accurate results.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0b9d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metropolis:\n",
    "    def __init__(self):\n",
    "        # Initialize chains\n",
    "        self.P_chain = []\n",
    "        self.m_chain = []\n",
    "        \n",
    "    def sample(self,m_0,log_posterior,h,n_samples,burnin=0,thin_factor=1):\n",
    "        # Compute initial unscaled log-posterior\n",
    "        P_0 = log_posterior(m_0)\n",
    "        \n",
    "        # Add initial location and posterior to the chain\n",
    "        self.P_chain.append(P_0)\n",
    "        self.m_chain.append(m_0)\n",
    "        n = len(m_0)\n",
    "\n",
    "        # Draw samples\n",
    "        for i in range(n_samples):\n",
    "\n",
    "\n",
    "            # Propose new value\n",
    "            m_prime = m_0 + np.random.randn(n)*h\n",
    "\n",
    "            # Compute new unscaled log-posterior\n",
    "            P_1 = log_posterior(m_prime)\n",
    "            \n",
    "            # Compute logarithm of probability ratio\n",
    "            log_ratio = P_1 - P_0\n",
    "            \n",
    "            # Convert to non-log space\n",
    "            ratio = np.exp(log_ratio)\n",
    "            \n",
    "            # If proposed value is more probable than current value, accept.  \n",
    "            # If not, then accept proportional to the probability ratios\n",
    "            if ratio>np.random.rand():\n",
    "                m_0 = m_prime\n",
    "                P_0 = P_1\n",
    "                \n",
    "            # Only append to the chain if we're past burn-in. \n",
    "            if i>burnin:\n",
    "                # Only append every j-th sample to the chain\n",
    "                if i%thin_factor==0:\n",
    "                    self.P_chain.append(P_0)\n",
    "                    self.m_chain.append(m_0)\n",
    "            \n",
    "            if i%100==0:\n",
    "                print(i, P_1)\n",
    "                    \n",
    "        return np.array(self.P_chain),np.array(self.m_chain)\n",
    "\n",
    "# Instantiate sampler\n",
    "sampler = Metropolis()\n",
    "\n",
    "class SIRPosterior:\n",
    "    def __init__(self,t_obs,u_obs,sigma2_obs):\n",
    "        self.u_obs = u_obs\n",
    "        self.t_obs = t_obs\n",
    "        self.sigma2_obs = sigma2_obs\n",
    "        \n",
    "    def log_posterior(self,log_m):\n",
    "        # We have defined our parameters to sample over as the logarithm\n",
    "        # of the model parameters.  Here we exponentiate them to get\n",
    "        # the representation that we need.  \n",
    "        m = np.exp(log_m)\n",
    "        S_0 = m[0]\n",
    "        I_0 = m[1]\n",
    "        R_0 = m[2]\n",
    "        beta = m[3]\n",
    "        gamma = m[4]\n",
    "        \n",
    "        u0 = np.array([S_0,I_0,R_0])\n",
    "\n",
    "        s = SIR(beta=beta,gamma=gamma)\n",
    "        integrator = om.Integrator(s,method)\n",
    "        t,u = integrator.integrate([times[0],times[-1]],1,u0)\n",
    "        P = -0.5*np.sum((self.u_obs - u)**2)/self.sigma2_obs\n",
    "        return P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa635e7",
   "metadata": {},
   "source": [
    "**Find a way to visualize the distribution of $k$ values.  Verify that these are sensible solutions by running the model forward with these values and ensuring that they match the observations consistent with the stated errors.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1278ab4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
